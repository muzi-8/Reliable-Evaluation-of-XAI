# Reliable-Evaluation-of-XAI
This repository is the official implementation and is a benchmark for comprehensive evaluation of model saliency/attention map explanations from  existing interpretability technique.
This repository contains the code, pre-trained model, constructed evaluation data set and ground truth by human annotation. We propose a unified evaluation framework for the 
three dimensions of accuracy, persuasibility and class-discriminativeness.
# Evaluation framework
![image](https://github.com/muzi-8/Reliable-Evaluation-of-XAI/blob/main/images/framework.PNG)
# Contents
## code
## pre-trained model
## constructed evaluation dataset
![image](https://github.com/muzi-8/Reliable-Evaluation-of-XAI/blob/main/images/dataset.PNG)
## Ground Truth
# Evaluation Process
## Overview of Accuracy Evaluation
![image](https://github.com/muzi-8/Reliable-Evaluation-of-XAI/blob/main/images/accuracy%20pipeline.PNG)
## Overview of  Persuasibility Evaluation
![iamge](https://github.com/muzi-8/Reliable-Evaluation-of-XAI/blob/main/images/persuasibility%20pipeline.PNG)
## Overview of Class-discriminativeness Evaluation
